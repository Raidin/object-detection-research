{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stpe-2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T08:26:56.775882Z",
     "start_time": "2020-01-30T08:26:55.315869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import pydot\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Model, optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import multi_gpu_model, plot_model, to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T08:27:29.330106Z",
     "start_time": "2020-01-30T08:27:29.307400Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-61028fd2e27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Train Data(with regions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning_research/object_detection/venv/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.npz'"
     ]
    }
   ],
   "source": [
    "# Load Train Data(with regions)\n",
    "train_data = np.load(os.path.join(experiments, 'train_data.npz'))\n",
    "print(train_data.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Augmenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T01:56:57.367821Z",
     "start_time": "2019-12-03T01:56:09.972856Z"
    }
   },
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "\n",
    "cls_trn_img = train_data['cls_trn_img']\n",
    "cls_trn_lb = to_categorical(train_data['cls_trn_lb'], 2)\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, validation_split=0.2)\n",
    "train_datagen.fit(cls_trn_img)\n",
    "\n",
    "# train-data\n",
    "train_generator = train_datagen.flow(cls_trn_img, cls_trn_lb, batch_size=128, shuffle=True, subset='training')\n",
    "# val-data\n",
    "validation_generator = train_datagen.flow(cls_trn_img, cls_trn_lb, batch_size=128, shuffle=True, subset='validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T01:57:10.681454Z",
     "start_time": "2019-12-03T01:56:37.626Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU Allocation Setting\n",
    "    # import tensorflow as tf\n",
    "    # from keras.backend.tensorflow_backend import set_session\n",
    "    # config = tf.ConfigProto()\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    # config.gpu_options.visible_device_list = '0'\n",
    "    # set_session(tf.Session(config=config))\n",
    "\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "for layers in (vggmodel.layers)[:15]:\n",
    "    layers.trainable = False\n",
    "    \n",
    "X = vggmodel.layers[-2].output\n",
    "predictions = Dense(2, activation=\"softmax\")(X)\n",
    "\n",
    "model_final = Model(input = vggmodel.input, output = predictions)\n",
    "\n",
    "# Multi GPU\n",
    "# model_final = multi_gpu_model(model_final, gpus=2)\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "model_final.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T01:57:10.689548Z",
     "start_time": "2019-12-03T01:56:47.682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Save\n",
    "model_json = model_final.to_json()\n",
    "with open('network_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# To Text File\n",
    "with open('network_model.txt', 'w') as model_file:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model_final.summary(print_fn=lambda x: model_file.write(x + '\\n'))\n",
    "\n",
    "# To Model Visualization\n",
    "# plot_model(model_final, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "log = CSVLogger('log.csv', append=True, separator=';')\n",
    "checkpoint = ModelCheckpoint(filepath='vgg16-airplane_{epoch:02d}_{val_loss:.4f}.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')\n",
    "callback_list = [log, checkpoint, early]\n",
    "\n",
    "history = model_final.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=int((cls_trn_img.shape[0] * (1.0 - 0.2)) / 128),\n",
    "    epochs=1000,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=int((cls_trn_img.shape[0] * 0.2) / 128),\n",
    "    verbose=1,\n",
    "    callbacks=callback_list)\n",
    "\n",
    "model_final.save_weights('trained_weight.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loss&Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T02:51:14.668198Z",
     "start_time": "2019-12-03T02:51:14.651584Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def VisualizeLossAccCurve(data):\n",
    "    fig, loss_ax = plt.subplots(1, figsize=(10, 10))\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    color_list = ['r', 'g', 'b', 'y']\n",
    "\n",
    "    loss_ax.plot(data['loss'], color=color_list[0], linestyle='-', label='loss')\n",
    "    acc_ax.plot(data['accuracy'], color=color_list[1], linestyle='-', label='accuracy')\n",
    "\n",
    "    loss_ax.plot(data['val_loss'], color=color_list[2], linestyle=':', label='val_loss')\n",
    "    acc_ax.plot(data['val_accuracy'], color=color_list[3], linestyle=':', label='val_accuracy')\n",
    "\n",
    "    loss_ax.set_xlabel('Epoch')\n",
    "    loss_ax.set_ylabel('Loss')\n",
    "    acc_ax.set_ylabel('Accuracy')\n",
    "\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    fig.suptitle('Training Results(Accuracy & Loss)', fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T02:51:15.348788Z",
     "start_time": "2019-12-03T02:51:14.850123Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# model 학습 종료된 파라미터 있을때.\n",
    "# VisualizeLossAccCurve(history.history)\n",
    "\n",
    "# 학습 log.csv file 읽어와서 사용 할 때\n",
    "job_name = 'vgg16-airplane'\n",
    "log_dir = os.path.join(os.getcwd(), 'experiments', job_name)\n",
    "log_path = os.path.join(log_dir, 'log.csv')\n",
    "data = pd.read_csv(log_path, sep=';')\n",
    "print('data shape ::', data.shape)\n",
    "print('data columns ::', data.columns)\n",
    "display(pd.DataFrame(data))\n",
    "data = data[['accuracy', 'loss', 'val_accuracy', 'val_loss']]\n",
    "\n",
    "VisualizeLossAccCurve(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 454,
   "position": {
    "height": "40px",
    "left": "594px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
